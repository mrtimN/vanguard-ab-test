{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eff3ea-d84c-4085-910d-5115e60c7a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# loading all datasets\n",
    "url_client_profiles = 'data/df_final_demo.txt'\n",
    "url_digital_footprints1 = 'data/df_final_web_data_pt_1.txt'\n",
    "url_digital_footprints2 = 'data/df_final_web_data_pt_2.txt'\n",
    "url_experiment_roster = 'data/df_final_experiment_clients.txt'\n",
    "\n",
    "df1 = pd.read_csv(url_digital_footprints1)\n",
    "df2 = pd.read_csv(url_digital_footprints2)\n",
    "\n",
    "# imported dataframes to work with\n",
    "df_client_profiles = pd.read_csv(url_client_profiles)\n",
    "df_exp_roster = pd.read_csv(url_experiment_roster)\n",
    "# merged footprint files\n",
    "df_footprints = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f497f4-a759-4fcc-8abd-6f996d63ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the datasets\n",
    "df_client_profiles.rename(columns={'clnt_tenure_yr': 'client_tenure_years', 'clnt_tenure_mnth': 'client_tenure_months', 'clnt_age': 'client_age', 'gendr': 'gender', 'num_accts': 'num_accounts', 'bal': 'balance', 'calls_6_mnth': 'calls_6months', 'logons_6_mnth': 'logins_6months'}, inplace=True)\n",
    "df_client_profiles_cleaned = df_client_profiles.dropna(subset=[\"client_tenure_years\", \"client_tenure_months\", \"client_age\", \"gender\", \"num_accounts\", \"balance\", \"calls_6months\", \"logins_6months\"], how=\"all\")\n",
    "df_exp_roster.rename(columns={'Variation': 'variation'}, inplace=True)\n",
    "df_exp_roster_cleaned = df_exp_roster.dropna(subset=[\"variation\"], how=\"all\")\n",
    "df_footprints_cleaned = df_footprints.dropna(subset=[\"client_id\", \"visitor_id\", \"visit_id\", \"process_step\", \"date_time\"], how=\"all\")\n",
    "df_footprints_cleaned = df_footprints_cleaned.drop_duplicates(subset=['client_id', 'visit_id', 'date_time'])\n",
    "df_client_profiles_cleaned['gender'] = df_client_profiles_cleaned['gender'].apply(lambda x: 'U' if x == 'X' else x)\n",
    "df_client_profiles_cleaned['gender'] = df_client_profiles_cleaned['gender'].fillna('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608569ba-f844-4326-9b32-6f1c3ee066b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining footprints and experiment roster datasets\n",
    "if not isinstance(locals().get('df_sorted'), pd.DataFrame):\n",
    "    df_footprints_cleaned.set_index('client_id', inplace=True)\n",
    "    df_exp_roster_cleaned.set_index('client_id', inplace=True)\n",
    "    joined_df = df_footprints_cleaned.join(df_exp_roster_cleaned, how='inner')\n",
    "    joined_df.reset_index(drop=False, inplace=True)\n",
    "    joined_df['date_time'] = pd.to_datetime(joined_df['date_time'])\n",
    "    \n",
    "df_sorted = joined_df.sort_values(by=['variation', 'visit_id', 'date_time'])\n",
    "df_first_confirm = df_sorted[df_sorted[\"process_step\"] == \"confirm\"].drop_duplicates(subset=\"visit_id\", keep=\"first\")\n",
    "df_no_confirms = df_sorted[df_sorted[\"process_step\"] != \"confirm\"]\n",
    "df_sorted = pd.concat([df_no_confirms, df_first_confirm]).sort_values(by=['variation', 'visit_id', 'date_time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea10f5-7ca7-46f7-a8e5-7989ca50caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date_time' is in datetime format\n",
    "df_sorted['date_time'] = pd.to_datetime(df_sorted['date_time'])\n",
    "\n",
    "# Sort by visit_id and date_time to ensure correct time difference calculation\n",
    "df_sorted = df_sorted.sort_values(by=['visit_id', 'date_time'])\n",
    "\n",
    "# Shift the 'date_time' column to get the next timestamp within each visit_id group\n",
    "df_sorted['next_date_time'] = df_sorted.groupby('visit_id')['date_time'].shift(-1)\n",
    "\n",
    "# Calculate time spent on each step (difference between the next timestamp and the current one)\n",
    "df_sorted['time_spent'] = (df_sorted['next_date_time'] - df_sorted['date_time']).dt.total_seconds()\n",
    "\n",
    "# Drop the temporary 'next_date_time' column as it's no longer needed\n",
    "df_sorted.drop(columns=['next_date_time'], inplace=True)\n",
    "\n",
    "# set 0 for NaN values (last steps)\n",
    "df_sorted['time_spent'] = df_sorted['time_spent'].fillna(0)\n",
    "\n",
    "# Compute the average time spent per process_step\n",
    "average_time_per_step = df_sorted.groupby('process_step')['time_spent'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c29a87-ef2f-4b83-a543-dbfca245018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Calculating Completion Rate per Group + Performing Hypothesis Testing #####\n",
    "\n",
    "# Create seperate dfs for treatment and control group to pass/ calculate the completion rate kpi \n",
    "df_test = joined_df[joined_df[\"variation\"] == \"Test\"]\n",
    "df_control = joined_df[joined_df[\"variation\"] == \"Control\"]\n",
    "\n",
    "# Count total unique visits per group\n",
    "n_control = df_control['visit_id'].nunique()\n",
    "n_test = df_test['visit_id'].nunique()\n",
    "\n",
    "# Count unique visits that reached the \"Confirm\" step per group\n",
    "completed_control = df_control[df_control['process_step'] == 'confirm']['visit_id'].nunique()\n",
    "completed_test = df_test[df_test['process_step'] == 'confirm']['visit_id'].nunique()\n",
    "\n",
    "# Calculate completion rates\n",
    "comp_rate_control = completed_control / n_control\n",
    "comp_rate_test = completed_test / n_test\n",
    "\n",
    "# Print completion rates\n",
    "print(f\"The completion rate for the control group (old version) is: {comp_rate_control:.4f}\")\n",
    "print(f\"The completion rate for the test group (new version) is: {comp_rate_test:.4f}\")\n",
    "\n",
    "# Perform Z-test for proportions\n",
    "count = np.array([completed_test, completed_control ])  # Successes in each group\n",
    "nobs = np.array([n_test, n_control])  # Total observations in each group\n",
    "\n",
    "z_stat, p_value = proportions_ztest(count, nobs, alternative='larger')\n",
    "\n",
    "# Print results\n",
    "print(f\"Z-statistic: {z_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the result\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: The new version has a significantly higher completion rate.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant difference in completion rates.\")\n",
    "\n",
    "# Claculating: Completion Rate with a Cost-Effectiveness Threshold\n",
    "threshold = 0.05  # 5% increase required\n",
    "observed_increase = comp_rate_test - comp_rate_control\n",
    "\n",
    "# Check if the observed increase meets the threshold\n",
    "if observed_increase >= threshold:\n",
    "    print(f\"The observed increase in completion rate is {observed_increase:.4f}, which meets or exceeds the required 5% threshold.\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Additionally, the increase is statistically significant. The new design is both effective and justifiable from a cost perspective.\")\n",
    "    else:\n",
    "        print(\"However, the increase is not statistically significant. Further analysis may be needed before making a final decision.\")\n",
    "else:\n",
    "    print(f\"The observed increase in completion rate is {observed_increase:.4f}, which is below the required 5% threshold.\")\n",
    "    print(\"The new design does not meet the cost-effectiveness criterion and may not justify the associated costs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c73bc2-edfd-45fa-aab5-71214047fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_previous_steps = {\n",
    "    'start': None,\n",
    "    'step_1': 'start',\n",
    "    'step_2': 'step_1',\n",
    "    'step_3': 'step_2',\n",
    "    'confirm': 'step_3'\n",
    "}\n",
    "\n",
    "def mark_errors(df, possible_previous_steps):\n",
    "    df = df.copy()  # Avoid modifying original DataFrame\n",
    "    \n",
    "    # Sort data to ensure correct order\n",
    "    df = df.sort_values(by=['visitor_id', 'visit_id', 'date_time'])\n",
    "    \n",
    "    # Shift previous step within each visit_id\n",
    "    df['previous_step'] = df.groupby(['visit_id'])['process_step'].shift(1)\n",
    "\n",
    "    # Define error condition\n",
    "    df['error'] = (df['previous_step'] != df['process_step'].map(possible_previous_steps)) & (df['previous_step'].notna() & (df['previous_step'] != df['process_step']))\n",
    "\n",
    "    return df.drop(columns=['previous_step'])\n",
    "\n",
    "df_with_errors = mark_errors(df_sorted, possible_previous_steps)\n",
    "\n",
    "# count errors and visits per variation\n",
    "error_counts = df_with_errors.groupby('variation')['error'].sum()\n",
    "visit_counts = df_with_errors.groupby('variation')['visit_id'].nunique()\n",
    "\n",
    "# extract values safely\n",
    "error_control, error_test = error_counts.get('Control', 0), error_counts.get('Test', 0)\n",
    "visit_control, visit_test = visit_counts.get('Control', 1), visit_counts.get('Test', 1)\n",
    "\n",
    "# compute error rates\n",
    "error_rate_control = error_control / visit_control\n",
    "error_rate_test = error_test / visit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe8ab8-7d89-4bc8-81dd-f82d6ba473a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define counts of errors and total visits for both groups\n",
    "errors = [error_control, error_test]\n",
    "visits = [visit_control, visit_test]\n",
    "\n",
    "# Hypothesis Testing\n",
    "# H0: The error rates for test group and control group are equal\n",
    "# H1: The error rate for test group is different (or higher) than the control group\n",
    "\n",
    "alpha = 0.05\n",
    "# Perform Z-test for proportions\n",
    "z_stat, p_value = proportions_ztest(errors, visits)\n",
    "\n",
    "print(f'Z_Stat: {z_stat}')\n",
    "print(f'P_value: {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8fad82-0652-4487-b82a-c7029f3d59e6",
   "metadata": {},
   "source": [
    "The p-value is 0.0, which is far below the common significance threshold of 0.05. This means we  reject the null hypothesis that the error rates for the test group and control group are equal.\n",
    "\n",
    "Since the test group's error rate is higher than the control group's, this suggests that the variation introduced in the test group significantly increased errors.\n",
    "\n",
    "Final Conclusion:\n",
    "The test group has a statistically significant higher error rate compared to the control group. This means the changes applied to the test group negatively impacted user experience by leading to more errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb061f-7733-466b-bcf6-2c029ae8231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_profiles_cleaned.groupby(['gender'])['client_age'].agg(['mean', 'median', 'min', 'max', 'count'])\n",
    "df_client_profiles_cleaned.groupby(['gender'])['client_tenure_years'].agg(['mean', 'median', 'min', 'max', 'count'])\n",
    "df_client_profiles_cleaned.groupby(['gender'])['balance'].agg(['mean', 'median', 'min', 'max', 'count'])\n",
    "\n",
    "# activity score calculation\n",
    "df_client_profiles_cleaned['activity_score'] = df_client_profiles_cleaned.iloc[:,8] + (df_client_profiles_cleaned.iloc[:,7]*0.5) + (df_client_profiles_cleaned.iloc[:,6]*0.00001)\n",
    "df_client_profiles_cleaned.sort_values('activity_score', ascending=False)\n",
    "\n",
    "#login and call scores\n",
    "high_login_threshold = df_client_profiles_cleaned['logins_6months'].quantile(0.8)\n",
    "low_login_threshold = df_client_profiles_cleaned['logins_6months'].quantile(0.2)\n",
    "high_call_threshold = df_client_profiles_cleaned['calls_6months'].quantile(0.8)\n",
    "low_call_threshold = df_client_profiles_cleaned['calls_6months'].quantile(0.2)\n",
    "\n",
    "def segment_customer_logins(logins):\n",
    "    if logins >= high_login_threshold:\n",
    "        return \"Highly Active\"\n",
    "    elif logins <= low_login_threshold:\n",
    "        return \"Inactive\"\n",
    "    else:\n",
    "        return \"Moderate\"\n",
    "\n",
    "def segment_customer_calls(calls):\n",
    "    if calls >= high_call_threshold:\n",
    "        return \"Highly Active\"\n",
    "    elif calls <= low_call_threshold:\n",
    "        return \"Inactive\"\n",
    "    else:\n",
    "        return \"Moderate\"\n",
    "\n",
    "df_client_profiles_cleaned[\"login_activity\"] = df_client_profiles_cleaned[\"logins_6months\"].apply(segment_customer_logins)\n",
    "df_client_profiles_cleaned[\"calls_activity\"] = df_client_profiles_cleaned[\"calls_6months\"].apply(segment_customer_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb65a94-f6c4-4c2a-a35e-1ce9de9836c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "login_activity_counts = df_client_profiles_cleaned[\"login_activity\"].value_counts()\n",
    "df_client_profiles_cleaned.groupby([\"login_activity\", \"gender\"])[[\"client_age\", \"balance\", \"client_tenure_years\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1158b7-c148-4866-bedd-68cbf52933dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls_activity_counts = df_client_profiles_cleaned[\"calls_activity\"].value_counts()\n",
    "df_client_profiles_cleaned.groupby([\"calls_activity\", \"gender\"])[[\"client_age\", \"balance\", \"client_tenure_years\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf02785-b959-4379-b5d6-971ecbb3659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting datatypes before exporting to csv\n",
    "df_client_profiles_cleaned['client_tenure_years'] = df_client_profiles_cleaned['client_tenure_years'].astype(int)\n",
    "df_client_profiles_cleaned['client_tenure_months'] = df_client_profiles_cleaned['client_tenure_months'].astype(int)\n",
    "df_client_profiles_cleaned['client_age'] = df_client_profiles_cleaned['client_age'].ffill()\n",
    "df_client_profiles_cleaned['client_age'] = df_client_profiles_cleaned['client_age'].astype(int)\n",
    "df_client_profiles_cleaned['num_accounts'] = df_client_profiles_cleaned['num_accounts'].astype(int)\n",
    "df_client_profiles_cleaned['calls_6months'] = df_client_profiles_cleaned['calls_6months'].astype(int)\n",
    "df_client_profiles_cleaned['logins_6months'] = df_client_profiles_cleaned['logins_6months'].astype(int)\n",
    "df_with_errors['time_spent'] = df_with_errors['time_spent'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2c142-fde1-4e8f-9c31-c912ac098710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_profiles_cleaned.to_csv('data/client_profiles.csv', index=False)\n",
    "df_with_errors.to_csv('data/abtest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace7a01-cc12-4ba8-9138-31b2a7540645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
